# Stochastic gradient descent (SGD)

SGD는 GD가 Local Minimum에 갇히는 한계를 해결하기 위해 개발된 기법이다.

물론 SGD는 항상 Global Minimum에 도달하는 것이 아니다. 단지 Local Minimum에 탈출 할 기회를 가진다는 것을 의미한다.

SGD에 대해서 자세히 설명해보자.

SGD는 GD에 Stochastic라는 단어가 앞에 붙어졌다.

왜냐하면 GD는 Loss를 계산시에 모든 데이터를 고려하지만 SGD는 무작위로 하나의 데이터를 선택하여 계산한다. 이러한 확률적 선택을 하기 때문에 Stochastic라는 단어가 붙어졌다.

다만, 주의할점은 무작위로 하나를 선택하되, 모든 데이터를 추출할때까지 비복원 추출을 진행한다.

모든 데이터를 추출했으면 전체 데이터를 복원하여 다시 과정을 반복한다. 이때 전체 데이터를 순회하는것을 1 Epoch이라고 부른다.

$$
{\displaystyle \mathbf {p} _{n+1}=\mathbf {p} _{n}-\gamma \nabla F(\mathbf {p} _{n})}
$$

GD의 수식과 비슷하지만, $F(x) = L = e_i$ , L은 하나의 데이터만 다룬다는게 특징이다.

이전에 말한 GD의 한계를 SGD가 해결하였는지 살펴보자.

> 1. GD를 계산할때 모든 데이터를 고려하여 그래디언트의 값을 계산하기 때문에 한 번 업데이트를 하는 과정 또한 시간이 많이 걸린다.

SGD는 하나의 데이터만 고려하여 그래디언트의 값을 계산하기에 GD보다 빠르게 방향을 정한다. 

그리고 전체 수렴을 하는 시간도 SGD가 더 빠르다.

> 2. Global Minimum이 아닌 Local Minimum에 빠질 수 있다.

SGD는 GD와 달리 하나의 데이터만 고려하여 최저점을 향해 진행하고, 무작위로 선택된 데이터에 따라 방향이 정해지기 때문에 GD와 달리 전체 Loss 관점에서 불규칙한 경로로 움직인다. 

GD는 현재 위치에서 가장 가까운 Minimum을 찾기 때문에 Local Minimum에 갇힐 확률이 매우 높지만, SGD는 전체 Loss 관점에서 데이터를 무작위로 선택한 순서에 따라 경로가 변하므로 Local Minimum에 탈출할 기회를 제공한다. 

그래서 복잡한 loss에는 GD보다 더 유연하게 탐색이 가능하며 Global Minimum을 찾을 확률을 높여준다.

## SGD (Stochastic gradient descent) 한계

1. 하나의 데이터를 고려하기 때문에, 대규모 데이터셋에서 편향된 업데이트를 발생한다.

SGD와 GD 모두 Loss를 활용하여 loss를 줄이는 것을 목표로 한다. 다만, 하나의 데이터로 loss를 결정하면 해당 데이터가 outlier이면 편향된 방향으로 업데이트하여 문제가 발생할수 있다.

쉽게 설명한다면, GD는 모든 데이터를 고려하므로 너무 신중하게 방향을 정해서 문제가 생기며, SGD는 하나의 데이터만 고려해서 고민 없이 방향을 정해서 문제가 생긴다는 것이다.

이러한 문제점을 해결하려면 데이터를 하나, 전체가 아닌 복수의 데이터를 계산하면 된다.

다만 복수의 데이터를 선택할때 전체의 개수로 향할수록 GD와 가까워지며, 개수가 적어질수록 SGD로 향할 것이다.

이러한 기법을 Mini-Batch Gardient descent라고 부른다.