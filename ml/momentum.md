# Momentum

GD, SGD, Mini-Batch GD는 현재 시점의 그래디언트를 고려하여 개선할 사항이 많습니다.

Momentum은 기존 알고리즘과 달리 과거의 그래디언트 정보를 활용하여 더 효과적인 학습을 수행합니다.

## 작동 원리

1. 첫번째 이동: GD와 동일한 방향으로 이동합니다.
2. 두번째 이동: 현재와 이전 그래디언트를 합산하여 방향을 이동합니다. 첫번째 이동이 right(<-))이었다면, 현재 그래디언트가 left(->)을 가리키면, 과거의 right 방향이 남아 있어서 left의 이동이 줄어듭니다.
3. 세번째 이동: 현재 그래디언트가 right방향으로 가리켜도 직전의 left 방향이 남아 있어 right 이동이 줄어듭니다.
4. 이후 이동: 과정이 반복되면서 좌우 움직임이 상쇄되지만, minimum으로 방향으로 움직임이 누적됩니다.

momentum은 불필요한 좌우 방향의 진동을 줄이며, minimum을 향해 더 빠르게 나아갑니다. 

다만, 관성으로 인해 minimum을 벗어날수 있지만, 전체적으로 더 효율적으로 학습이 가능하게 됩니다.