# LeNet-5

> **Gradient-based learning applied to document recognition**  
> Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner (1998)

LeNet-5는 손글씨 숫자 인식을 위해 제안된 Convolutional Neural Network(CNN)로,  
**국소 수용장**, **가중치 공유**, **서브샘플링**이라는 세 가지 핵심 아이디어를 결합하여  
한정된 파라미터로도 뛰어난 일반화 성능을 달성했습니다.

## 전체 아키텍처

![LeNet-5 Architecture](/assets/img/lenet-5.png)

1. **입력**: 32 × 32 gray-level 이미지  
   - 원본 데이터셋의 문자는 20 × 20 크기, 28 × 28 필드 중앙에 배치  
   - LeNet-5는 32 × 32 캔버스로 여백을 줘서 “획 끝”이나 “모서리” 같은 중요한 특징이 항상 수용장의 중앙에 오도록 보장  

2. **정규화**:  
   - 배경(흰색) → 0.1, 전경(검은색) → 1.175 스케일링  
   - 입력 평균 ≈ 0, 분산 ≈ 1 → 학습 가속화

## 레이어 표기

- Cx : convolutional layers
- Sx : Subsampling layers
- Fx : Fully-connected layers

x는 해당 계층의 인덱스입니다.

## Layer Details

| Layer | Type                   | Maps/Units | RF Size     | Output   | Params | Connections |
|:-----:|:----------------------:|:----------:|:------------|:---------|:------:|:-----------:|
| C1    | Conv (valid)           | 6 @ 28×28  | 5×5         | 28×28    | 156    | 122,304     |
| S2    | Subsample (2×2, stride=2) | 6 @ 14×14  | 2×2 avg + scale & bias | 14×14 | 12     | 5,880       |
| C3    | Conv (partial connect) | 16 @ 10×10 | 5×5 on S2   | 10×10    | 1,516  | 156,000     |
| S4    | Subsample (2×2)        | 16 @ 5×5   | 2×2 avg + scale & bias | 5×5   | 32     | 4,000       |
| C5    | Conv (->1×1)            | 120 @ 1×1  | 5×5 on S4   | 1×1      | 48,120 | 120         |
| F6    | Fully-connected        | 84         | —           | 84       | 10,164 | —           |
| Out   | RBF classifier         | 10         | —           | 10       | fixed | —           |

---

### C3 부분 연결 패턴

| 그룹 | 맵 인덱스 | 연결된 S2 맵 수 | Weights per map | Bias per map | 그룹 파라미터 합계 |
|:----:|:---------:|:--------------:|:---------------:|:-----------:|:------------------:|
| 1    | 0–5       | 3              | 3×5×5 = 75      | 1           | 6×(75+1)=456       |
| 2    | 6–11      | 4              | 4×5×5 = 100     | 1           | 6×(100+1)=606      |
| 3    | 12–14     | 4              | 4×5×5 = 100     | 1           | 3×(100+1)=303      |
| 4    | 15        | 6              | 6×5×5 = 150     | 1           | 1×(150+1)=151      |
|      | **합계**  | —              | —               | —           | **1,516**          |

![](/assets/img/lenet-table1.png)  
*Table 1. C3의 부분 연결 세부 구성.*

C3은 “부분 연결” 구조를 취함으로써 **파라미터 수를 절약하면서도 다양한 공간적 조합의 특징을 학습할 수 있는 장점이 있습니다.**

### 유닛 연산 및 활성화 함수

- 연산: 입력 벡터와 가중치 벡터의 내적 + 바이어스 -> 
- 활성화: $y = 1.7159tanh(2x/3)$
- 함수는 odd, 수평 점근선 ±1.7159

이 스케일링은 학습 안정성과 수렴 속도를 높이기 위해 선택됨

### 출력 계층
- 유닛 종류: 클래스당 하나씩, 총 10개의 Euclidean RBF 유닛
- 입력 수: 각 84개(층 F6)
- 출력: 입력 벡터과 파라미터 벡터 간 유클리드 거리 → 음의 로그가능도(penalty) 해석
- 파라미터 벡터: 7×12 비트맵으로 그린 문자 모양(±1로 수동 설정)
    - 유사 클래스(예: “O” vs “0”)는 유사한 파라미터 코드 → 혼동 최소화 

## 손실 함수 (Loss Function)

가장 간단한 출력 손실 함수는 **최대 우도 추정(maximum likelihood estimation)** 기준이며, 본 논문에서는 **평균 제곱 오차(minimum mean squared error, MSE)** 와 동일합니다. 주어진 학습 샘플 집합 $\{(x^{(p)},\,t^{(p)})\}$에 대한 손실 함수는 다음과 같습니다:

$$
{J = \frac{1}{2}\sum_{p}\sum_{i}\bigl(y_i^{(p)} - t_i^{(p)}\bigr)^2}
$$

* 여기서 $y_i^{(p)}$는 패턴 $p$의 **정답 클래스**에 해당하는 $i$번째 RBF 유닛의 출력입니다.

하지만 이 MSE 기준에는 **세 가지 중요한 단점**이 있습니다:

1. **‘붕괴(collapse)’ 현상**

   * 만약 RBF 매개변수를 학습하도록 허용하면, 모든 RBF 중심 벡터가 동일해지고 F6 계층의 상태 역시 그 벡터로 고정되는 자명하지만 용납할 수 없는 해가 존재합니다.
   * 이 경우 네트워크는 입력을 완전히 무시하고 모든 RBF 출력이 0이 되어 버립니다.
   * (참고로, RBF 가중치를 고정하면 이 현상은 발생하지 않습니다.)

2. **클래스 간 경쟁 부재**

   * MSE 기준은 올바른 클래스 패널티만 낮출 뿐, 잘못된 클래스 패널티를 같이 **높여 주는 경쟁 메커니즘**이 없습니다.
   * 이를 해결하기 위해 **사후 확률 최대화(maximum a posteriori, MAP)** 기준을 도입할 수 있으며, 이는 HMM 훈련에 쓰이는 **최대 상호정보(maximum mutual information)** 기준과 유사합니다.
   * MAP 기준에 따른 손실 함수는 다음과 같이 쓸 수 있습니다:

$$
  {L = -ln P(\text{정답 클래스}\mid x)\propto E_{\text{correct}} +\ln\sum_{c}\exp\bigl(-E_{c}\bigr)}
$$

   여기서 $E_c$는 클래스 $c$에 대응하는 RBF 유닛의 패널티(출력)입니다.
   * 두 번째 항( $\ln\sum_{c}\exp(-E_c)$ )이 **경쟁 항**으로 작동하며, 이미 큰 패널티가 지나치게 커지는 것을 막아 줍니다.
   * 이 기준을 쓰면 RBF 중심 벡터가 자연스럽게 서로 멀어져 붕괴 현상이 방지됩니다.

3. **‘쓰레기(rubbish)’ 클래스 포함**

   * 입력이 어느 클래스든, 또는 배경(“rubbish”) 클래스에서 올 수 있다고 가정하고 학습함으로써,
   * 잘못된 클래스 패널티를 함께 끌어올려 **더욱 구분력 있는** 판별 경계를 학습할 수 있습니다.
   * 이 방식의 다중 객체(예: 단어, 문장) 분류 문제로의 일반화된 형태는 **Section VI**에서 상세히 다룹니다.

마지막으로, 이 손실 함수의 기울기를 네트워크의 모든 가중치에 대해 계산하려면 **역전파(back-propagation)** 기법을 사용합니다.

* **가중치 공유** 구조를 고려하기 위해,

  1. 먼저 **공유 없이** 각 연결(connection)에 대한 편미분을 계산하고
  2. 같은 파라미터를 공유하는 모든 연결의 미분을 합산하여 최종 파라미터 미분을 얻습니다.


## 학습 설정

- **Optimizer**: Stochastic Diagonal Levenberg–Marquardt  
- **Learning Rate** (epoch별):  
    - Epoch 1–2: 5e-4
    - Epoch 3–5: 2e-4
    - Epoch 6–8: 1e-4
    - Epoch 9–12: 5e-5
    - Epoch 13+: 1e-5

- **Batch size**: 1 (온라인 업데이트), 60,000 패턴 × 20 에폭
